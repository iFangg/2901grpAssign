%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Lachaise Assignment
% LaTeX Template
% Version 1.0 (26/6/2018)
%
% This template originates from:
% http://www.LaTeXTemplates.com
%
% Authors:
% Marion Lachaise & François Févotte
% Vel (vel@LaTeXTemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass{article}

\usepackage{parskip}
\usepackage{amsmath}
\usepackage{amsthm}

\DeclareMathOperator{\Ima}{im}

\input{structure.tex} % Include the file specifying the document structure and custom commands

%----------------------------------------------------------------------------------------
%	ASSIGNMENT INFORMATION
%----------------------------------------------------------------------------------------

\title{Math2901: Group Assignment} % Title of the assignment

\author{Dylan Wang, Ivan Fang\\ \texttt{z5422214@ad.unsw.edu.au, z5418045@ad.unsw.edu.au}} % Author name and email address

\date{University of New South Wales --- \today} % University, school and/or department name(s) and a date

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle % Print the title

%----------------------------------------------------------------------------------------
%	INTRODUCTION
%----------------------------------------------------------------------------------------

\section*{Question 1 Solutions} % Unnumbered section
Let $A, B\subseteq\Omega$.

1. 

Given that event $A$ is independent of itself, this means that $\mathbb{P}(A|A) = \mathbb{P}(A)$. This implies $\mathbb{P}(A) = \mathbb{P}(A)$ and so $\mathbb{P}(A)$ must equal either $1$ or $0$.

2.

Given event $A$ such that $\mathbb{P}(A) = 1$ or $\mathbb{P}(A) = 0$, we observe the conditional probability between events $A$ and $B$.

For the case where $\mathbb{P}(A) = 1$:
\begin{align*}
    \mathbb{P}(B|A) &= \frac{\mathbb{P}(B\cap A)}{\mathbb{P}(A)}\\
    &= \mathbb{P}(B\cap A)\\
    &= \mathbb{P}(B)\mbox{.}
\end{align*}
\hspace*{6mm}We reach this result because the intersection with a guaranteed event will always be the probability of the other event.

For the case where $\mathbb{P}(A) = 0$:
\begin{align*}
    \mathbb{P}(A|B) &= \frac{\mathbb{P}(A\cap B)}{\mathbb{P}(B)}\\
    &= \frac{\mathbb{P}(A) + \mathbb{P}(B) - \mathbb{P}(A\cup B)}{\mathbb{P}(B)}\\
    &= \frac{\mathbb{P}(B) - \mathbb{P}(A\cup B)}{\mathbb{P}(B)}\\
    &= \frac{\mathbb{P}(B) - \mathbb{P}(B)}{\mathbb{P}(B)}\\
    &= 0\\
    &= \mathbb{P}(A)\mbox{.}
\end{align*}
\hspace*{6mm} Since the union with an impossible event always returns the probability of the other event, we reach our result.

3.

Note, by the Total Law of Probability, $\mathbb{P}(A\cap B) \leq 1$.

Now observe,
\begin{align*}
    \mathbb{P}(A\cap B) & = \mathbb{P}(A) +\mathbb{P}(B) - \mathbb{P}(A\cap B)\\    
    1 &\geq \mathbb{P}(A) +\mathbb{P}(B) - \mathbb{P}(A\cap B)\\
    \mathbb{P}(A\cap B)\ &\geq \mathbb{P}(A) +\mathbb{P}(B) - 1\mbox{.}
\end{align*}
\hspace*{6mm} The inequality has been proven, thus we are done.

4.

Using the previous proven inequality,
\begin{align*}
    \mathbb{P}(A_1\cap A_2)\ &\geq \mathbb{P}(A_1) + \mathbb{P}(A_2) - 1\\
    \mathbb{P}(A_1\cap A_2\cap A_3)\ &\geq \mathbb{P}(A_1) + \mathbb{P}(A_2) - 1\\
    &= \mathbb{P}(A_1) +\mathbb{P}(A_2) - 1 +  \mathbb{P}(A_3) - 1\\
    &= \mathbb{P}(A_1) +\mathbb{P}(A_2) +  \mathbb{P}(A_3) - 2\\
    &\vdots\\
    \mathbb{P}(\bigcap_{i = 1}^n A_i)\ &\geq \sum_{i = 1}^{n} A_i - (n-1)\mbox{.}
\end{align*}
\hspace*{6mm} We have thus proved the inequality. 

\section*{Question 2 Solutions} % Unnumbered section


\section*{Question 3 Solutions} % Unnumbered section
1.

We can see that $\tilde{X_n} = \bar{X_n} + \frac{1}{n}(2X_1)-\frac{1}{n}\left(X_{n-1}+X_n\right)$. Now calculating the bias of $\tilde{X_n}$
\begin{align*}
    \mbox{Bias}(\tilde{X_n}) &=\mbox{E}(\tilde{X_n}) - \mu\\
    &= \frac{1}{n}\mbox{E}\left(\sum_{i = 1}^{n}X_i + 2X_1 - (X_{n-1}+X_n)\right) - \mu\\
    &= \frac{1}{n}\mbox{E}\left(\sum_{i = 1}^{n}X_i\right)+\frac{1}{n}\mbox{E}(2X_1)-\frac{1}{n}\mbox{E}(X_{n-1}+X_n) - \mu\\
    &= \frac{1}{n}\mbox{E}\left(\sum_{i = 1}^{n}X_i\right)+\frac{2}{n}\mbox{E}(X_1)-\frac{1}{n}\mbox{E}(X_{n-1})-\frac{1}{n}\mbox{E}(X_n) - \mu\mbox{.}
\end{align*}
\hspace*{6mm}Because the random sample is independent identically distributed, the expected value of each variable in the sample is equal to the mean. This means that our simplified equation becomes
\begin{align*}
    \mu + \frac{2}{n}\mu -\frac{1}{n}\mu -\frac{1}{n}\mu -\mu = 0\mbox{.}
\end{align*}
We have thus shown that $\tilde{X_n}$ is an unbiased estimator of $\mu$.

2.

The mean square error can be written as
\begin{align*}
    \mbox{MSE}(\tilde{X_n}) &= \mbox{Var}(\tilde{X_n})+(\mbox{Bias}(\tilde{X_n}))^2\\
    &= \mbox{Var}(\tilde{X_n})\\
    &= \mbox{Var}\left(\frac{3X_1+\sum_{i = 2}^{n-2}X_i}{n}\right)\\
    &= \frac{1}{n^2}\left(9\mbox{Var}(X_1) + \sum_{i = 2}^{n-2}\mbox{Var}(X_i)\right)\\
    &= \frac{9\sigma^2 + \sigma^2(n - 3)}{n}\\
    &= \frac{6\sigma^2 + n\sigma^2}{n^2}\mbox{.}
\end{align*}
Thus, the MSE is $\frac{1}{n^2}(6\sigma^2 + n\sigma^2)$.

3.
\begin{align*}
    \lim_{n\rightarrow\infty}\mbox{MSE}(\tilde{X_n}) &= \lim_{n\rightarrow\infty}\frac{1}{n^2}(6\sigma^2 + n\sigma^2)\\
    &= 0\mbox{.}
\end{align*}

4.

To measure the better estimate of $\mu$, we can observe the variances of each
\begin{equation*}
    \mbox{Var}(\bar{X_n}) = \frac{\sigma^2}{n}\leq \frac{1}{n^2}(6\sigma^2 + n\sigma^2) = \mbox{Var}(\tilde{X_n})\mbox{.}
\end{equation*}
Since the variance of $\bar{X_n}$ is lesser, it is the better estimator.

\section*{Question 4 Solutions} % Unnumbered section
\textbf{Part I:}

a)
Null Hypothesis: $H_0\mbox{: }\mu = 5$

Alternative Hypothesis: $H_1\mbox{: }\mu \neq 5$

b)

\textbf{Part II:}

a)

\end{document}
